{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c064b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they wandered into a strange tiki bar on the edge of the small beach town every manager should be able to recite at least ten nursery rhymes backward find bar near beach\n",
      "iguanas wandered were falling out of the trees the sunblock was handed to the girl before practice but the burned skin was proof she did not apply it better she must have used sunblock\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "text1 = \"They wandered into a strange Tiki bar on the edge of the small beach town. Every manager should be able to recite at least ten nursery rhymes backward. Find bar near beach\"\n",
    "text2 = \"Iguanas wandered were falling out of the trees. The sunblock was handed to the girl before practice, but the burned skin was proof she did not apply it. Better she must have used sunblock\"\n",
    "text1 = text1.lower()\n",
    "text2 = text2.lower()\n",
    "text1 = text1.translate(str.maketrans('','',string.punctuation))\n",
    "text2 = text2.translate(str.maketrans('','',string.punctuation))\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f49f5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they wandered into a strange tiki bar on the edge of the small beach town every manager should be able to recite at least ten nursery rhymes backward find bar near beach']\n",
      "['iguanas wandered were falling out of the trees the sunblock was handed to the girl before practice but the burned skin was proof she did not apply it better she must have used sunblock']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenized_text1 = nltk.sent_tokenize(text1)\n",
    "tokenized_text2 = nltk.sent_tokenize(text2)\n",
    "print(tokenized_text1)\n",
    "print(tokenized_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b214d3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have', 'such', 'just', 'themselves', 'doing', 'are', 'further', 'hers', 'myself', 'all', 'were', 'y', 'while', 'on', \"hasn't\", 'they', 'own', 'where', 'ma', 'now', 'more', 'herself', 'don', 'this', \"wouldn't\", \"you're\", \"hadn't\", 'then', 'he', 'why', 'both', 'over', 'd', 'haven', 'shan', 'wouldn', 'me', 'below', 'any', 'doesn', 'himself', 'my', 'out', \"aren't\", 'll', 'by', 'from', 'is', 'because', \"couldn't\", 'whom', \"don't\", 'had', \"needn't\", 'other', 'here', 'been', 'about', 'in', 'as', 'some', \"isn't\", \"won't\", 's', \"should've\", 'too', \"shouldn't\", 'those', 'to', \"didn't\", 'under', 'each', 'm', 'shouldn', 'with', 'am', 'having', 'mightn', 'ain', 'your', 'these', 'has', 'yourselves', 'which', 'between', \"she's\", 'weren', 'our', 'we', 'you', 'be', 'at', 'ourselves', \"you'd\", 'hadn', 'above', 't', \"you've\", \"doesn't\", 'the', 'isn', 'yourself', 'before', \"that'll\", 'she', 'after', 'off', \"haven't\", \"mightn't\", 'mustn', 'them', \"wasn't\", 'or', 'did', \"mustn't\", 'there', 'and', \"shan't\", 'does', 'until', 'its', 'so', 'being', 'not', 'no', 'him', 'will', \"weren't\", 'against', 'their', 'itself', 'yours', 'of', 'but', 'once', 'o', 'few', 'down', \"you'll\", 'couldn', 'when', 'an', 'if', 'same', 'should', 're', 'how', 'that', 'who', 'his', 'again', 'into', 'was', 'it', \"it's\", 'a', 'won', 'ours', 'aren', 'what', 'i', 'nor', 'through', 'theirs', 'needn', 'didn', 'do', 'wasn', 'for', 'can', 'than', 'most', 'during', 'hasn', 'her', 'up', 've', 'only', 'very'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fab7e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Output after Stop Words Removal Text 1*****\n",
      "\n",
      "['wandered', 'strange', 'tiki', 'bar', 'edge', 'small', 'beach', 'town', 'every', 'manager', 'able', 'recite', 'least', 'ten', 'nursery', 'rhymes', 'backward', 'find', 'bar', 'near', 'beach']\n",
      "\n",
      "\n",
      "****Output after Stop Words Removal Text 2*****\n",
      "\n",
      "['iguanas', 'wandered', 'falling', 'trees', 'sunblock', 'handed', 'girl', 'practice', 'burned', 'skin', 'proof', 'apply', 'better', 'must', 'used', 'sunblock']\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal\n",
    "filtered1 = []\n",
    "filtered2 = []\n",
    "for i in tokenized_text1:\n",
    "    wordlist = nltk.word_tokenize(i)\n",
    "    for j in wordlist:\n",
    "        if not j in stopWords:\n",
    "            filtered1.append(j)\n",
    "for i in tokenized_text2:\n",
    "    wordlist = nltk.word_tokenize(i)\n",
    "    for j in wordlist:\n",
    "        if not j in stopWords:\n",
    "            filtered2.append(j)\n",
    "\n",
    "print(\"****Output after Stop Words Removal Text 1*****\\n\")\n",
    "print(filtered1)\n",
    "print(\"\\n\")\n",
    "print(\"****Output after Stop Words Removal Text 2*****\\n\")\n",
    "print(filtered2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f19a60db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Stemmed text 1*****\n",
      "\n",
      "{'wandered': 'wander', 'strange': 'strang', 'tiki': 'tiki', 'bar': 'bar', 'edge': 'edg', 'small': 'small', 'beach': 'beach', 'town': 'town', 'every': 'everi', 'manager': 'manag', 'able': 'abl', 'recite': 'recit', 'least': 'least', 'ten': 'ten', 'nursery': 'nurseri', 'rhymes': 'rhyme', 'backward': 'backward', 'find': 'find', 'near': 'near'}\n",
      "\n",
      "\n",
      "*****Stemmed text 2*****\n",
      "\n",
      "{'iguanas': 'iguana', 'wandered': 'wander', 'falling': 'fall', 'trees': 'tree', 'sunblock': 'sunblock', 'handed': 'hand', 'girl': 'girl', 'practice': 'practic', 'burned': 'burn', 'skin': 'skin', 'proof': 'proof', 'apply': 'appli', 'better': 'better', 'must': 'must', 'used': 'use'}\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "stemmed1 = {}\n",
    "stemmed2 = {}\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for i in filtered1:\n",
    "    stemmed1.update({i : ps.stem(i)})\n",
    "for i in filtered2:\n",
    "    stemmed2.update({i : ps.stem(i)})\n",
    "print(\"*****Stemmed text 1*****\\n\")\n",
    "print(stemmed1)\n",
    "print(\"\\n\")\n",
    "print(\"*****Stemmed text 2*****\\n\")\n",
    "print(stemmed2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "374920b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Lemmetized Text 1*****\n",
      "\n",
      "{'wandered': 'wandered', 'strange': 'strange', 'tiki': 'tiki', 'bar': 'bar', 'edge': 'edge', 'small': 'small', 'beach': 'beach', 'town': 'town', 'every': 'every', 'manager': 'manager', 'able': 'able', 'recite': 'recite', 'least': 'least', 'ten': 'ten', 'nursery': 'nursery', 'rhymes': 'rhyme', 'backward': 'backward', 'find': 'find', 'near': 'near'}\n",
      "\n",
      "\n",
      "******Lemmetized Text 2*****\n",
      "\n",
      "{'iguanas': 'iguanas', 'wandered': 'wandered', 'falling': 'falling', 'trees': 'trees', 'sunblock': 'sunblock', 'handed': 'handed', 'girl': 'girl', 'practice': 'practice', 'burned': 'burned', 'skin': 'skin', 'proof': 'proof', 'apply': 'apply', 'better': 'good', 'must': 'must', 'used': 'used'}\n"
     ]
    }
   ],
   "source": [
    "#lemmitization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmed1 = {}\n",
    "lemmed2 = {}\n",
    "for i in filtered1:\n",
    "    lemmed1.update({i:lemmatizer.lemmatize(i)})\n",
    "    \n",
    "for i in filtered2:\n",
    "    lemmed2.update({i:lemmatizer.lemmatize(i, pos=\"a\")})\n",
    "    \n",
    "print(\"******Lemmetized Text 1*****\\n\")\n",
    "print(lemmed1)\n",
    "print(\"\\n\")\n",
    "print(\"******Lemmetized Text 2*****\\n\")\n",
    "print(lemmed2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5322940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****POS OF TEXT 1\n",
      "\n",
      "[('wandered', 'JJ'), ('strange', 'JJ'), ('tiki', 'NN'), ('bar', 'NN'), ('edge', 'VBP'), ('small', 'JJ'), ('beach', 'NN'), ('town', 'NN'), ('every', 'DT'), ('manager', 'NN'), ('able', 'JJ'), ('recite', 'NN'), ('least', 'JJS'), ('ten', 'JJ'), ('nursery', 'JJ'), ('rhymes', 'NNS'), ('backward', 'RB'), ('find', 'VBP'), ('bar', 'NN'), ('near', 'IN'), ('beach', 'NN')]\n",
      "****POS OF TEXT 2\n",
      "\n",
      "[('iguanas', 'NNS'), ('wandered', 'VBD'), ('falling', 'VBG'), ('trees', 'NNS'), ('sunblock', 'RB'), ('handed', 'VBD'), ('girl', 'JJ'), ('practice', 'NN'), ('burned', 'VBD'), ('skin', 'JJ'), ('proof', 'NN'), ('apply', 'VB'), ('better', 'JJR'), ('must', 'MD'), ('used', 'VBN'), ('sunblock', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Parts Of Speech\n",
    "pos1 = []\n",
    "pos2 = []\n",
    "print(\"****POS OF TEXT 1\\n\")\n",
    "print(nltk.pos_tag(filtered1))\n",
    "print(\"****POS OF TEXT 2\\n\")\n",
    "print(nltk.pos_tag(filtered2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61c8af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency for text 1\n",
      "\n",
      "{'wandered': 0.047619047619047616, 'strange': 0.047619047619047616, 'tiki': 0.047619047619047616, 'bar': 0.09523809523809523, 'edge': 0.047619047619047616, 'small': 0.047619047619047616, 'beach': 0.09523809523809523, 'town': 0.047619047619047616, 'every': 0.047619047619047616, 'manager': 0.047619047619047616, 'able': 0.047619047619047616, 'recite': 0.047619047619047616, 'least': 0.047619047619047616, 'ten': 0.047619047619047616, 'nursery': 0.047619047619047616, 'rhymes': 0.047619047619047616, 'backward': 0.047619047619047616, 'find': 0.047619047619047616, 'near': 0.047619047619047616}\n",
      "\n",
      "Term Frequency for text 2\n",
      "\n",
      "{'iguanas': 0.0625, 'wandered': 0.0625, 'falling': 0.0625, 'trees': 0.0625, 'sunblock': 0.125, 'handed': 0.0625, 'girl': 0.0625, 'practice': 0.0625, 'burned': 0.0625, 'skin': 0.0625, 'proof': 0.0625, 'apply': 0.0625, 'better': 0.0625, 'must': 0.0625, 'used': 0.0625}\n"
     ]
    }
   ],
   "source": [
    "term_frequency1 = {}\n",
    "term_frequency2 = {}\n",
    "for i in filtered1:\n",
    "    term_frequency1.update({i:(filtered1.count(i))/len(filtered1)})\n",
    "for i in filtered2:\n",
    "    term_frequency2.update({i:(filtered2.count(i))/len(filtered2)})\n",
    "print(\"Term Frequency for text 1\\n\")\n",
    "print(term_frequency1)\n",
    "print(\"\\nTerm Frequency for text 2\\n\")\n",
    "print(term_frequency2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5970016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Doucment frequency of Document1\n",
      "\n",
      "{'wandered': 0.0, 'strange': 0.6931471805599453, 'tiki': 0.6931471805599453, 'bar': 0.6931471805599453, 'edge': 0.6931471805599453, 'small': 0.6931471805599453, 'beach': 0.6931471805599453, 'town': 0.6931471805599453, 'every': 0.6931471805599453, 'manager': 0.6931471805599453, 'able': 0.6931471805599453, 'recite': 0.6931471805599453, 'least': 0.6931471805599453, 'ten': 0.6931471805599453, 'nursery': 0.6931471805599453, 'rhymes': 0.6931471805599453, 'backward': 0.6931471805599453, 'find': 0.6931471805599453, 'near': 0.6931471805599453}\n",
      "\n",
      "Inverse Doucment frequency of Document2\n",
      "\n",
      "{'iguanas': 0.6931471805599453, 'wandered': 0.0, 'falling': 0.6931471805599453, 'trees': 0.6931471805599453, 'sunblock': 0.6931471805599453, 'handed': 0.6931471805599453, 'girl': 0.6931471805599453, 'practice': 0.6931471805599453, 'burned': 0.6931471805599453, 'skin': 0.6931471805599453, 'proof': 0.6931471805599453, 'apply': 0.6931471805599453, 'better': 0.6931471805599453, 'must': 0.6931471805599453, 'used': 0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "ifd1 = {}\n",
    "ifd2 = {}\n",
    "for i in filtered1:\n",
    "    cnt = 0;\n",
    "    if filtered1.count(i) > 0:\n",
    "        cnt += 1\n",
    "    if filtered2.count(i) > 0:\n",
    "        cnt += 1\n",
    "    f = math.log(2/cnt)\n",
    "    ifd1.update({i:f})\n",
    "    \n",
    "for i in filtered2:\n",
    "    cnt = 0;\n",
    "    if filtered1.count(i) > 0:\n",
    "        cnt += 1\n",
    "    if filtered2.count(i) > 0:\n",
    "        cnt += 1\n",
    "    f = math.log(2/cnt)\n",
    "    ifd2.update({i:f})\n",
    "\n",
    "print(\"Inverse Doucment frequency of Document1\\n\")\n",
    "print(ifd1)\n",
    "print(\"\\nInverse Doucment frequency of Document2\\n\")\n",
    "print(ifd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f826a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
